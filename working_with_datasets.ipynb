{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/almutareb/huggingface-nlp-demo/blob/main/working_with_datasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWYQcOujwhQv"
      },
      "source": [
        "🤗 Datasets provides loading scripts to handle the loading of local and remote datasets. It supports several common data formats, such as:\n",
        "\n",
        "* CSV & TSV (csv) : \tload_dataset(\"csv\", data_files=\"my_file.csv\")\n",
        "* Text files \t(text) : \tload_dataset(\"text\", data_files=\"my_file.txt\")\n",
        "* JSON & JSON Lines (json) : \tload_dataset(\"json\", data_files=\"my_file.jsonl\")\n",
        "* Pickled DataFrames (pandas) : \tload_dataset(\"pandas\", data_files=\"my_dataframe.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGZxHFKrw21F"
      },
      "source": [
        "For this example we’ll use the SQuAD-it dataset, which is a large-scale dataset for question answering in Italian.\n",
        "\n",
        "The training and test splits are hosted on GitHub, so we can download them with a simple wget command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DwjqJIyd5O1"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-train.json.gz\n",
        "!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-test.json.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0oqeECYw-OH"
      },
      "outputs": [],
      "source": [
        "!gzip -dkv SQuAD_it-*.json.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTQ4r0ydxG0q"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwKKvMu-xPjf"
      },
      "source": [
        "To load a JSON file with the load_dataset() function, we just need to know if we’re dealing with ordinary JSON (similar to a nested dictionary) or JSON Lines (line-separated JSON). Like many question answering datasets, SQuAD-it uses the nested format, with all the text stored in a data field. This means we can load the dataset by specifying the field argument as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vSbp53dxUsM"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "squad_it_dataset = load_dataset(\"json\", data_files=\"SQuAD_it-train.json\", field=\"data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcxq7oFtzbO2"
      },
      "source": [
        "By default, loading local files creates a DatasetDict object with a train split. We can see this by inspecting the squad_it_dataset object:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBGrGzPOzb6m"
      },
      "outputs": [],
      "source": [
        "squad_it_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUgdPfCczjNQ"
      },
      "outputs": [],
      "source": [
        "squad_it_dataset[\"train\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEe9dABxzzYD"
      },
      "source": [
        "Great, we’ve loaded our first local dataset! But while this worked for the training set, what we really want is to include both the train and test splits in a single DatasetDict object so we can apply Dataset.map() functions across both splits at once. To do this, we can provide a dictionary to the data_files argument that maps each split name to a file associated with that split:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEVniSjwz300"
      },
      "outputs": [],
      "source": [
        "data_files = {\"train\": \"SQuAD_it-train.json\", \"test\": \"SQuAD_it-test.json\"}\n",
        "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")\n",
        "squad_it_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ama4lQfp1ZDN"
      },
      "source": [
        "Now, we can apply various preprocessing techniques to clean up the data, tokenize the reviews, and so on.\n",
        "\n",
        "The loading scripts in 🤗 Datasets actually support automatic decompression of the input files, so we could have skipped the use of gzip by pointing the data_files argument directly to the compressed files:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6YTir-z1fEc"
      },
      "outputs": [],
      "source": [
        "data_files = {\"train\": \"SQuAD_it-train.json.gz\", \"test\": \"SQuAD_it-test.json.gz\"}\n",
        "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYTSfCMN1oPD"
      },
      "source": [
        "This can be useful if you don’t want to manually decompress many GZIP files. The automatic decompression also applies to other common formats like ZIP and TAR, so you just need to point data_files to the compressed files and you’re good to go!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ti05uCa2s8t"
      },
      "source": [
        "Loading remote files is just as simple as loading local ones! Instead of providing a path to local files, we point the data_files argument of load_dataset() to one or more URLs where the remote files are stored. For example, for the SQuAD-it dataset hosted on GitHub, we can just point data_files to the SQuAD_it-*.json.gz URLs as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBozpWzp22gy"
      },
      "outputs": [],
      "source": [
        "url = \"https://github.com/crux82/squad-it/raw/master/\"\n",
        "data_files = {\n",
        "    \"train\": url + \"SQuAD_it-train.json.gz\",\n",
        "    \"test\": url + \"SQuAD_it-test.json.gz\",\n",
        "}\n",
        "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxSholIt3O_x"
      },
      "outputs": [],
      "source": [
        "squad_it_dataset[\"test\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8UTJCeX3pHc"
      },
      "source": [
        "Most of the time, the data you work with won’t be perfectly prepared for training models. In the following section we’ll explore the various features that 🤗 Datasets provides to clean up your datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mU_G9F03lTe"
      },
      "source": [
        "# Slicing and dicing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-0CN-l53wXg"
      },
      "source": [
        "Similar to Pandas, 🤗 Datasets provides several functions to manipulate the contents of Dataset and DatasetDict objects.\n",
        "\n",
        "For this example we’ll use the Drug Review Dataset that’s hosted on the UC Irvine Machine Learning Repository, which contains patient reviews on various drugs, along with the condition being treated and a 10-star rating of the patient’s satisfaction.\n",
        "\n",
        "First we need to download and extract the data, which can be done with the wget and unzip commands:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ssz_ti737TR"
      },
      "outputs": [],
      "source": [
        "!wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip\"\n",
        "!unzip drugsCom_raw.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHYAz2xK4giA"
      },
      "source": [
        "Since TSV is just a variant of CSV that uses tabs instead of commas as the separator, we can load these files by using the csv loading script and specifying the delimiter argument in the load_dataset() function as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uc_-HAsy4hP9"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "data_files = {\"train\": \"drugsComTrain_raw.tsv\", \"test\": \"drugsComTest_raw.tsv\"}\n",
        "# \\t is the tab character in Python\n",
        "drug_dataset = load_dataset(\"csv\", data_files=data_files, delimiter=\"\\t\")\n",
        "drug_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BR7C7K7y49m0"
      },
      "source": [
        "A good practice when doing any sort of data analysis is to grab a small random sample to get a quick feel for the type of data you’re working with. In 🤗 Datasets, we can create a random sample by chaining the Dataset.shuffle() and Dataset.select() functions together:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0OfkYSp4-Kb"
      },
      "outputs": [],
      "source": [
        "drug_sample = drug_dataset[\"train\"].shuffle(seed=42).select(range(100))\n",
        "# peek ar the first 3 examples\n",
        "drug_sample[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fELKFRP16I02"
      },
      "source": [
        "From this sample we can already see a few quirks in our dataset:\n",
        "\n",
        "* The Unnamed: 0 column looks suspiciously like an anonymized ID for each patient.\n",
        "* The condition column includes a mix of uppercase and lowercase labels.\n",
        "* The reviews are of varying length and contain a mix of Python line separators (\\r\\n) as well as HTML character codes like &\\#039;.\n",
        "\n",
        "Let’s see how we can use 🤗 Datasets to deal with each of these issues. To test the patient ID hypothesis for the Unnamed: 0 column, we can use the Dataset.unique() function to verify that the number of IDs matches the number of rows in each split:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VaszVg7Q6TD3"
      },
      "outputs": [],
      "source": [
        "for split in drug_dataset.keys():\n",
        "  assert len(drug_dataset[split]) == len(drug_dataset[split].unique(\"Unnamed: 0\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4ogBojw6wML"
      },
      "source": [
        "This seems to confirm our hypothesis, so let’s clean up the dataset a bit by renaming the Unnamed: 0 column to something a bit more interpretable. We can use the DatasetDict.rename_column() function to rename the column across both splits in one go:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5HVnpqr96x8m"
      },
      "outputs": [],
      "source": [
        "drug_dataset = drug_dataset.rename_column(\n",
        "    original_column_name=\"Unnamed: 0\", new_column_name=\"patient_id\"\n",
        ")\n",
        "drug_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pf8f1sDt-lYE"
      },
      "source": [
        "number of unique drugs and conditions in the training and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3T0ZxKUF8doN"
      },
      "outputs": [],
      "source": [
        "len(drug_dataset[\"train\"].unique(\"drugName\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bp7YmKZ6-pJ5"
      },
      "outputs": [],
      "source": [
        "len(drug_dataset[\"train\"].unique(\"condition\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ysmg9rR78clL"
      },
      "source": [
        "In the 🤗 Datasets context, we can use lambda functions to define simple map and filter operations, so let’s use this trick to eliminate the None entries in our dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2CGGo1q8d8g"
      },
      "outputs": [],
      "source": [
        "drug_dataset = drug_dataset.filter(lambda x: x[\"condition\"] is not None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGSVZfFY89UO"
      },
      "source": [
        "In Python, lambda functions are small functions that you can define without explicitly naming them. They take the general form:\n",
        "\n",
        "> lambda \\<arguments> : \\<expression>\n",
        "\n",
        "where lambda is one of Python’s special keywords, \\<arguments> is a list/set of comma-separated values that define the inputs to the function, and \\<expression> represents the operations you wish to execute."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShLy49NJ-1LH"
      },
      "source": [
        "With the None entries removed, we can normalize our condition column using Dataset.map(). We can define a simple function that can be applied across all the rows of each split in drug_dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkg4mgbx-_Sa"
      },
      "outputs": [],
      "source": [
        "def lowercase_condition(example):\n",
        "  return {\"condition\": example[\"condition\"].lower()}\n",
        "\n",
        "drug_dataset.map(lowercase_condition)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJ0R-QyC9XQg"
      },
      "source": [
        "Now that we’ve cleaned up the labels, let’s take a look at cleaning up the reviews themselves."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Des14Mf3-ErW"
      },
      "source": [
        "# Creating new columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QS2RShrE-PKV"
      },
      "source": [
        "Whenever you’re dealing with customer reviews, a good practice is to check the number of words in each review. A review might be just a single word like “Great!” or a full-blown essay with thousands of words, and depending on the use case you’ll need to handle these extremes differently. To compute the number of words in each review, we’ll use a rough heuristic based on splitting each text by whitespace.\n",
        "\n",
        "Let’s define a simple function that counts the number of words in each review:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5AUVeAl-P0P"
      },
      "outputs": [],
      "source": [
        "def compute_review_length(example):\n",
        "  return {\"review_length\": len(example[\"review\"].split())}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8J29VbnC-dtM"
      },
      "source": [
        "Unlike our lowercase_condition() function, compute_review_length() returns a dictionary whose key does not correspond to one of the column names in the dataset. In this case, when compute_review_length() is passed to Dataset.map(), it will be applied to all the rows in the dataset to create a new review_length column:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZAbIvKM-iai"
      },
      "outputs": [],
      "source": [
        "drug_dataset = drug_dataset.map(compute_review_length)\n",
        "# inspect the first training example\n",
        "drug_dataset[\"train\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QShcRhjc-6P9"
      },
      "source": [
        "We can sort this new column with Dataset.sort() to see what the extreme values look like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3PQOIRA-77f"
      },
      "outputs": [],
      "source": [
        "drug_dataset[\"train\"].sort(\"review_length\")[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtCVRDMGAFAR"
      },
      "source": [
        "Looks like some reviews contain just a single word, which, although it may be okay for sentiment analysis, would not be informative if we want to predict the condition.\n",
        "\n",
        "Let’s use the Dataset.filter() function to remove reviews that contain fewer than 30 words. Similarly to what we did with the condition column, we can filter out the very short reviews by requiring that the reviews have a length above this threshold:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srxCbfk6ANGw"
      },
      "outputs": [],
      "source": [
        "drug_dataset = drug_dataset.filter(lambda x: x[\"review_length\"] > 30)\n",
        "print(drug_dataset.num_rows)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIt6JK7vAE2e"
      },
      "source": [
        "This has removed around 15% of the reviews from our original training and test sets.\n",
        "\n",
        "The last thing we need to deal with is the presence of HTML character codes in our reviews. We can use Python’s html module to unescape these characters, like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jc8N3UtdDD2R"
      },
      "outputs": [],
      "source": [
        "import html\n",
        "\n",
        "text = \"I&#039;m a transformer called BERT\"\n",
        "html.unescape(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqaA84smDP1Y"
      },
      "source": [
        "We’ll use Dataset.map() to unescape all the HTML characters in our corpus:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Miz25KoDDRGO"
      },
      "outputs": [],
      "source": [
        "drug_dataset = drug_dataset.map(lambda x: {\"review\": html.unescape(x[\"review\"])})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HfrTMU9M8nq"
      },
      "source": [
        "The Dataset.map() method takes a batched argument that, if set to True, causes it to send a batch of examples to the map function at once (the batch size is configurable but defaults to 1,000).\n",
        "\n",
        "When you specify batched=True the function receives a dictionary with the fields of the dataset, but each value is now a list of values, and not just a single value. The return value of Dataset.map() should be the same: a dictionary with the fields we want to update or add to our dataset, and a list of values. For example, here is another way to unescape all HTML characters, but using batched=True:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8DuZ1bGMniH"
      },
      "outputs": [],
      "source": [
        "new_drug_dataset = drug_dataset.map(\n",
        "    lambda x: {\"review\": [html.unescape(o) for o in x[\"review\"]]}, batched=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMX29CQINTnb"
      },
      "source": [
        "this command executes way faster than the previous one. And it’s not because our reviews have already been HTML-unescaped.\n",
        "\n",
        "Using Dataset.map() with batched=True will be essential to unlock the speed of the “fast” tokenizers, which can quickly tokenize big lists of texts. For instance, to tokenize all the drug reviews with a fast tokenizer, we could use a function like this:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "tYAl8jYZnRBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMzysD-ENeoD"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "  return tokenizer(examples[\"review\"], truncation=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s take this opportunity to compare the performance of the different options. In a notebook, you can time a one-line instruction by adding %time before the line of code you wish to measure:"
      ],
      "metadata": {
        "id": "m9_idsndnkh6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " %time tokenized_dataset = drug_dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "LQlKKZ53nq84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also time a whole cell by putting %%time at the beginning of the cell."
      ],
      "metadata": {
        "id": "KG0884yPoQyh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parallelization is also the reason for the nearly 6x speedup the fast tokenizer achieves with batching: you can’t parallelize a single tokenization operation, but when you want to tokenize lots of texts at the same time you can just split the execution across several processes, each responsible for its own texts.\n",
        "\n",
        "Dataset.map() also has some parallelization capabilities of its own. Since they are not backed by Rust, they won’t let a slow tokenizer catch up with a fast one, but they can still be helpful (especially if you’re using a tokenizer that doesn’t have a fast version). To enable multiprocessing, use the num_proc argument and specify the number of processes to use in your call to Dataset.map():"
      ],
      "metadata": {
        "id": "BzRkZ6GipnPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "slow_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\", use_fast=False)\n",
        "\n",
        "def slow_tokenize_function(examples):\n",
        "  return slow_tokenizer(examples[\"review\"], truncation=True)\n",
        "\n",
        "tokenized_dataset = drug_dataset.map(slow_tokenize_function, batched=True, num_proc=8)"
      ],
      "metadata": {
        "id": "7ddvhH5RpoV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In general, we don’t recommend using Python multiprocessing for fast tokenizers with batched=True.\n",
        "\n",
        "Using num_proc to speed up your processing is usually a great idea, as long as the function you are using is not already doing some kind of multiprocessing of its own.\n",
        "\n",
        "All of this functionality condensed into a single method is already pretty amazing, but there’s more! With Dataset.map() and batched=True you can change the number of elements in your dataset. This is super useful in many situations where you want to create several training features from one example."
      ],
      "metadata": {
        "id": "Sgn0E1Flq2K-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_split(examples):\n",
        "  return tokenizer(\n",
        "      examples[\"review\"],\n",
        "      truncation=True,\n",
        "      max_length=128,\n",
        "      return_overflowing_tokens=True,\n",
        "  )"
      ],
      "metadata": {
        "id": "rnzJPQ8bsaMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test the function on one example first, before using map to apply it to the whole dataset:"
      ],
      "metadata": {
        "id": "MUiUVT5IsvTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = tokenize_and_split(drug_dataset[\"train\"][0])\n",
        "[len(inp) for inp in result[\"input_ids\"]]"
      ],
      "metadata": {
        "id": "433ovKihs6ru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, our first example in the training set became two features because it was tokenized to more than the maximum number of tokens we specified: the first one of length 128 and the second one of length 49. Now let’s do this for all elements of the dataset!"
      ],
      "metadata": {
        "id": "oEXnODu7tZBz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset = drug_dataset.map(tokenize_and_split, batched=True)"
      ],
      "metadata": {
        "id": "YRJTQGuktZ32"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOVhbwvG6PhUgHFSMBn3HJB",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}