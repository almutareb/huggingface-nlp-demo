{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO10mtf3hNmM06W5vYlEjv6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/almutareb/huggingface-nlp-demo/blob/main/working_with_datasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ¤— Datasets provides loading scripts to handle the loading of local and remote datasets. It supports several common data formats, such as:\n",
        "\n",
        "* CSV & TSV (csv) : \tload_dataset(\"csv\", data_files=\"my_file.csv\")\n",
        "* Text files \t(text) : \tload_dataset(\"text\", data_files=\"my_file.txt\")\n",
        "* JSON & JSON Lines (json) : \tload_dataset(\"json\", data_files=\"my_file.jsonl\")\n",
        "* Pickled DataFrames (pandas) : \tload_dataset(\"pandas\", data_files=\"my_dataframe.pkl\")"
      ],
      "metadata": {
        "id": "WWYQcOujwhQv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this example weâ€™ll use the SQuAD-it dataset, which is a large-scale dataset for question answering in Italian.\n",
        "\n",
        "The training and test splits are hosted on GitHub, so we can download them with a simple wget command:"
      ],
      "metadata": {
        "id": "JGZxHFKrw21F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DwjqJIyd5O1"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-train.json.gz\n",
        "!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-test.json.gz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gzip -dkv SQuAD_it-*.json.gz"
      ],
      "metadata": {
        "id": "l0oqeECYw-OH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "sTQ4r0ydxG0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To load a JSON file with the load_dataset() function, we just need to know if weâ€™re dealing with ordinary JSON (similar to a nested dictionary) or JSON Lines (line-separated JSON). Like many question answering datasets, SQuAD-it uses the nested format, with all the text stored in a data field. This means we can load the dataset by specifying the field argument as follows:"
      ],
      "metadata": {
        "id": "OwKKvMu-xPjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "squad_it_dataset = load_dataset(\"json\", data_files=\"SQuAD_it-train.json\", field=\"data\")"
      ],
      "metadata": {
        "id": "5vSbp53dxUsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By default, loading local files creates a DatasetDict object with a train split. We can see this by inspecting the squad_it_dataset object:"
      ],
      "metadata": {
        "id": "kcxq7oFtzbO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "squad_it_dataset"
      ],
      "metadata": {
        "id": "hBGrGzPOzb6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "squad_it_dataset[\"train\"][0]"
      ],
      "metadata": {
        "id": "KUgdPfCczjNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great, weâ€™ve loaded our first local dataset! But while this worked for the training set, what we really want is to include both the train and test splits in a single DatasetDict object so we can apply Dataset.map() functions across both splits at once. To do this, we can provide a dictionary to the data_files argument that maps each split name to a file associated with that split:"
      ],
      "metadata": {
        "id": "YEe9dABxzzYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_files = {\"train\": \"SQuAD_it-train.json\", \"test\": \"SQuAD_it-test.json\"}\n",
        "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")\n",
        "squad_it_dataset"
      ],
      "metadata": {
        "id": "rEVniSjwz300"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can apply various preprocessing techniques to clean up the data, tokenize the reviews, and so on.\n",
        "\n",
        "The loading scripts in ðŸ¤— Datasets actually support automatic decompression of the input files, so we could have skipped the use of gzip by pointing the data_files argument directly to the compressed files:"
      ],
      "metadata": {
        "id": "ama4lQfp1ZDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_files = {\"train\": \"SQuAD_it-train.json.gz\", \"test\": \"SQuAD_it-test.json.gz\"}\n",
        "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")"
      ],
      "metadata": {
        "id": "H6YTir-z1fEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This can be useful if you donâ€™t want to manually decompress many GZIP files. The automatic decompression also applies to other common formats like ZIP and TAR, so you just need to point data_files to the compressed files and youâ€™re good to go!"
      ],
      "metadata": {
        "id": "BYTSfCMN1oPD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading remote files is just as simple as loading local ones! Instead of providing a path to local files, we point the data_files argument of load_dataset() to one or more URLs where the remote files are stored. For example, for the SQuAD-it dataset hosted on GitHub, we can just point data_files to the SQuAD_it-*.json.gz URLs as follows:"
      ],
      "metadata": {
        "id": "5ti05uCa2s8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://github.com/crux82/squad-it/raw/master/\"\n",
        "data_files = {\n",
        "    \"train\": url + \"SQuAD_it-train.json.gz\",\n",
        "    \"test\": url + \"SQuAD_it-test.json.gz\",\n",
        "}\n",
        "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")"
      ],
      "metadata": {
        "id": "RBozpWzp22gy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "squad_it_dataset[\"test\"][0]"
      ],
      "metadata": {
        "id": "FxSholIt3O_x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}