{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOPifgIyBHtKS0R/D6ErSiy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/almutareb/huggingface-nlp-demo/blob/main/working_with_datasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "🤗 Datasets provides loading scripts to handle the loading of local and remote datasets. It supports several common data formats, such as:\n",
        "\n",
        "* CSV & TSV (csv) : \tload_dataset(\"csv\", data_files=\"my_file.csv\")\n",
        "* Text files \t(text) : \tload_dataset(\"text\", data_files=\"my_file.txt\")\n",
        "* JSON & JSON Lines (json) : \tload_dataset(\"json\", data_files=\"my_file.jsonl\")\n",
        "* Pickled DataFrames (pandas) : \tload_dataset(\"pandas\", data_files=\"my_dataframe.pkl\")"
      ],
      "metadata": {
        "id": "WWYQcOujwhQv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this example we’ll use the SQuAD-it dataset, which is a large-scale dataset for question answering in Italian.\n",
        "\n",
        "The training and test splits are hosted on GitHub, so we can download them with a simple wget command:"
      ],
      "metadata": {
        "id": "JGZxHFKrw21F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DwjqJIyd5O1"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-train.json.gz\n",
        "!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-test.json.gz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gzip -dkv SQuAD_it-*.json.gz"
      ],
      "metadata": {
        "id": "l0oqeECYw-OH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "sTQ4r0ydxG0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To load a JSON file with the load_dataset() function, we just need to know if we’re dealing with ordinary JSON (similar to a nested dictionary) or JSON Lines (line-separated JSON). Like many question answering datasets, SQuAD-it uses the nested format, with all the text stored in a data field. This means we can load the dataset by specifying the field argument as follows:"
      ],
      "metadata": {
        "id": "OwKKvMu-xPjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "squad_it_dataset = load_dataset(\"json\", data_files=\"SQuAD_it-train.json\", field=\"data\")"
      ],
      "metadata": {
        "id": "5vSbp53dxUsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By default, loading local files creates a DatasetDict object with a train split. We can see this by inspecting the squad_it_dataset object:"
      ],
      "metadata": {
        "id": "kcxq7oFtzbO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "squad_it_dataset"
      ],
      "metadata": {
        "id": "hBGrGzPOzb6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "squad_it_dataset[\"train\"][0]"
      ],
      "metadata": {
        "id": "KUgdPfCczjNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great, we’ve loaded our first local dataset! But while this worked for the training set, what we really want is to include both the train and test splits in a single DatasetDict object so we can apply Dataset.map() functions across both splits at once. To do this, we can provide a dictionary to the data_files argument that maps each split name to a file associated with that split:"
      ],
      "metadata": {
        "id": "YEe9dABxzzYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_files = {\"train\": \"SQuAD_it-train.json\", \"test\": \"SQuAD_it-test.json\"}\n",
        "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")\n",
        "squad_it_dataset"
      ],
      "metadata": {
        "id": "rEVniSjwz300"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can apply various preprocessing techniques to clean up the data, tokenize the reviews, and so on.\n",
        "\n",
        "The loading scripts in 🤗 Datasets actually support automatic decompression of the input files, so we could have skipped the use of gzip by pointing the data_files argument directly to the compressed files:"
      ],
      "metadata": {
        "id": "ama4lQfp1ZDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_files = {\"train\": \"SQuAD_it-train.json.gz\", \"test\": \"SQuAD_it-test.json.gz\"}\n",
        "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")"
      ],
      "metadata": {
        "id": "H6YTir-z1fEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This can be useful if you don’t want to manually decompress many GZIP files. The automatic decompression also applies to other common formats like ZIP and TAR, so you just need to point data_files to the compressed files and you’re good to go!"
      ],
      "metadata": {
        "id": "BYTSfCMN1oPD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading remote files is just as simple as loading local ones! Instead of providing a path to local files, we point the data_files argument of load_dataset() to one or more URLs where the remote files are stored. For example, for the SQuAD-it dataset hosted on GitHub, we can just point data_files to the SQuAD_it-*.json.gz URLs as follows:"
      ],
      "metadata": {
        "id": "5ti05uCa2s8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://github.com/crux82/squad-it/raw/master/\"\n",
        "data_files = {\n",
        "    \"train\": url + \"SQuAD_it-train.json.gz\",\n",
        "    \"test\": url + \"SQuAD_it-test.json.gz\",\n",
        "}\n",
        "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")"
      ],
      "metadata": {
        "id": "RBozpWzp22gy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "squad_it_dataset[\"test\"][0]"
      ],
      "metadata": {
        "id": "FxSholIt3O_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most of the time, the data you work with won’t be perfectly prepared for training models. In the following section we’ll explore the various features that 🤗 Datasets provides to clean up your datasets."
      ],
      "metadata": {
        "id": "Q8UTJCeX3pHc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Slicing and dicing the data"
      ],
      "metadata": {
        "id": "-mU_G9F03lTe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to Pandas, 🤗 Datasets provides several functions to manipulate the contents of Dataset and DatasetDict objects.\n",
        "\n",
        "For this example we’ll use the Drug Review Dataset that’s hosted on the UC Irvine Machine Learning Repository, which contains patient reviews on various drugs, along with the condition being treated and a 10-star rating of the patient’s satisfaction.\n",
        "\n",
        "First we need to download and extract the data, which can be done with the wget and unzip commands:"
      ],
      "metadata": {
        "id": "p-0CN-l53wXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip\"\n",
        "!unzip drugsCom_raw.zip"
      ],
      "metadata": {
        "id": "1ssz_ti737TR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since TSV is just a variant of CSV that uses tabs instead of commas as the separator, we can load these files by using the csv loading script and specifying the delimiter argument in the load_dataset() function as follows:"
      ],
      "metadata": {
        "id": "uHYAz2xK4giA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "data_files = {\"train\": \"drugsComTrain_raw.tsv\", \"test\": \"drugsComTest_raw.tsv\"}\n",
        "# \\t is the tab character in Python\n",
        "drug_dataset = load_dataset(\"csv\", data_files=data_files, delimiter=\"\\t\")\n",
        "drug_dataset"
      ],
      "metadata": {
        "id": "Uc_-HAsy4hP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A good practice when doing any sort of data analysis is to grab a small random sample to get a quick feel for the type of data you’re working with. In 🤗 Datasets, we can create a random sample by chaining the Dataset.shuffle() and Dataset.select() functions together:"
      ],
      "metadata": {
        "id": "BR7C7K7y49m0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drug_sample = drug_dataset[\"train\"].shuffle(seed=42).select(range(100))\n",
        "# peek ar the first 3 examples\n",
        "drug_sample[:3]"
      ],
      "metadata": {
        "id": "_0OfkYSp4-Kb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From this sample we can already see a few quirks in our dataset:\n",
        "\n",
        "* The Unnamed: 0 column looks suspiciously like an anonymized ID for each patient.\n",
        "* The condition column includes a mix of uppercase and lowercase labels.\n",
        "* The reviews are of varying length and contain a mix of Python line separators (\\r\\n) as well as HTML character codes like &\\#039;.\n",
        "\n",
        "Let’s see how we can use 🤗 Datasets to deal with each of these issues. To test the patient ID hypothesis for the Unnamed: 0 column, we can use the Dataset.unique() function to verify that the number of IDs matches the number of rows in each split:"
      ],
      "metadata": {
        "id": "fELKFRP16I02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for split in drug_dataset.keys():\n",
        "  assert len(drug_dataset[split]) == len(drug_dataset[split].unique(\"Unnamed: 0\"))"
      ],
      "metadata": {
        "id": "VaszVg7Q6TD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This seems to confirm our hypothesis, so let’s clean up the dataset a bit by renaming the Unnamed: 0 column to something a bit more interpretable. We can use the DatasetDict.rename_column() function to rename the column across both splits in one go:"
      ],
      "metadata": {
        "id": "A4ogBojw6wML"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drug_dataset = drug_dataset.rename_column(\n",
        "    original_column_name=\"Unnamed: 0\", new_column_name=\"patient_id\"\n",
        ")\n",
        "drug_dataset"
      ],
      "metadata": {
        "id": "5HVnpqr96x8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "number of unique drugs and conditions in the training and test sets."
      ],
      "metadata": {
        "id": "pf8f1sDt-lYE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(drug_dataset[\"train\"].unique(\"drugName\"))"
      ],
      "metadata": {
        "id": "3T0ZxKUF8doN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(drug_dataset[\"train\"].unique(\"condition\"))"
      ],
      "metadata": {
        "id": "bp7YmKZ6-pJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the 🤗 Datasets context, we can use lambda functions to define simple map and filter operations, so let’s use this trick to eliminate the None entries in our dataset:"
      ],
      "metadata": {
        "id": "Ysmg9rR78clL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drug_dataset = drug_dataset.filter(lambda x: x[\"condition\"] is not None)"
      ],
      "metadata": {
        "id": "Y2CGGo1q8d8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Python, lambda functions are small functions that you can define without explicitly naming them. They take the general form:\n",
        "\n",
        "> lambda \\<arguments> : \\<expression>\n",
        "\n",
        "where lambda is one of Python’s special keywords, \\<arguments> is a list/set of comma-separated values that define the inputs to the function, and \\<expression> represents the operations you wish to execute."
      ],
      "metadata": {
        "id": "mGSVZfFY89UO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the None entries removed, we can normalize our condition column using Dataset.map(). We can define a simple function that can be applied across all the rows of each split in drug_dataset:"
      ],
      "metadata": {
        "id": "ShLy49NJ-1LH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lowercase_condition(example):\n",
        "  return {\"condition\": example[\"condition\"].lower()}\n",
        "\n",
        "drug_dataset.map(lowercase_condition)"
      ],
      "metadata": {
        "id": "nkg4mgbx-_Sa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we’ve cleaned up the labels, let’s take a look at cleaning up the reviews themselves."
      ],
      "metadata": {
        "id": "mJ0R-QyC9XQg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating new columns"
      ],
      "metadata": {
        "id": "Des14Mf3-ErW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Whenever you’re dealing with customer reviews, a good practice is to check the number of words in each review. A review might be just a single word like “Great!” or a full-blown essay with thousands of words, and depending on the use case you’ll need to handle these extremes differently. To compute the number of words in each review, we’ll use a rough heuristic based on splitting each text by whitespace.\n",
        "\n",
        "Let’s define a simple function that counts the number of words in each review:"
      ],
      "metadata": {
        "id": "QS2RShrE-PKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_review_length(example):\n",
        "  return {\"review_length\": len(example[\"review\"].split())}"
      ],
      "metadata": {
        "id": "l5AUVeAl-P0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unlike our lowercase_condition() function, compute_review_length() returns a dictionary whose key does not correspond to one of the column names in the dataset. In this case, when compute_review_length() is passed to Dataset.map(), it will be applied to all the rows in the dataset to create a new review_length column:"
      ],
      "metadata": {
        "id": "8J29VbnC-dtM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drug_dataset = drug_dataset.map(compute_review_length)\n",
        "# inspect the first training example\n",
        "drug_dataset[\"train\"][0]"
      ],
      "metadata": {
        "id": "BZAbIvKM-iai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can sort this new column with Dataset.sort() to see what the extreme values look like:"
      ],
      "metadata": {
        "id": "QShcRhjc-6P9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drug_dataset[\"train\"].sort(\"review_length\")[:3]"
      ],
      "metadata": {
        "id": "y3PQOIRA-77f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looks like some reviews contain just a single word, which, although it may be okay for sentiment analysis, would not be informative if we want to predict the condition.\n",
        "\n",
        "Let’s use the Dataset.filter() function to remove reviews that contain fewer than 30 words. Similarly to what we did with the condition column, we can filter out the very short reviews by requiring that the reviews have a length above this threshold:"
      ],
      "metadata": {
        "id": "PtCVRDMGAFAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drug_dataset = drug_dataset.filter(lambda x: x[\"review_length\"] > 30)\n",
        "print(drug_dataset.num_rows)"
      ],
      "metadata": {
        "id": "srxCbfk6ANGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This has removed around 15% of the reviews from our original training and test sets.\n",
        "\n",
        "The last thing we need to deal with is the presence of HTML character codes in our reviews. We can use Python’s html module to unescape these characters, like so:"
      ],
      "metadata": {
        "id": "hIt6JK7vAE2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import html\n",
        "\n",
        "text = \"I&#039;m a transformer called BERT\"\n",
        "html.unescape(text)"
      ],
      "metadata": {
        "id": "Jc8N3UtdDD2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We’ll use Dataset.map() to unescape all the HTML characters in our corpus:"
      ],
      "metadata": {
        "id": "iqaA84smDP1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drug_dataset = drug_dataset.map(lambda x: {\"review\": html.unescape(x[\"review\"])})"
      ],
      "metadata": {
        "id": "Miz25KoDDRGO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}