{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/almutareb/huggingface-nlp-demo/blob/main/fine_tune_a_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61raxwSZOhLa"
      },
      "outputs": [],
      "source": [
        "!pip install transformers[sentencepiece] datasets evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhYgAcUiPxX0"
      },
      "source": [
        "# Loading a dataset from HF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIxO7-qBLlM9"
      },
      "source": [
        "The ðŸ¤— Datasets library provides a very simple command to download and cache a dataset on the Hub. We can download the MRPC dataset like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ig8aQveLWL1"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "from datasets import load_dataset\n",
        "\n",
        "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
        "raw_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uufDcx5XkYiU"
      },
      "outputs": [],
      "source": [
        "raw_datasets_sst = load_dataset(\"glue\", \"sst2\")\n",
        "raw_datasets_sst"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41Lb26uMMSnp"
      },
      "source": [
        "We get a DatasetDict object which contains the training set, the validation set, and the test set. Each of those contains several columns (sentence1, sentence2, label, and idx) and a variable number of elements (there are 3,668 pairs of sentences in the training set, 408 in the validation set, and 1,725 in the test set).\n",
        "\n",
        "We can access each pair of sentences in our raw_datasets object by indexing, like with a dictionary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-a9JlREkMS0i"
      },
      "outputs": [],
      "source": [
        "raw_train_dataset = raw_datasets[\"train\"]\n",
        "raw_train_dataset[86]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XVfDK8rkvsz"
      },
      "outputs": [],
      "source": [
        "raw_train_dataset_sst = raw_datasets_sst[\"train\"]\n",
        "raw_train_dataset_sst[88]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAWuhbvUPTea"
      },
      "source": [
        "To know which integer corresponds to which label, we can inspect the features of our raw_train_dataset. This will tell us the type of each column:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRImuTebPULF"
      },
      "outputs": [],
      "source": [
        "raw_train_dataset.features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtAyfspkjd2B"
      },
      "outputs": [],
      "source": [
        "raw_train_dataset_sst.features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OXoJxQ-P1f-"
      },
      "source": [
        "# Preprocessing a dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_rtmqYgP5O7"
      },
      "source": [
        "To preprocess the dataset, we need to convert the text to numbers the model can make sense of. We can feed the tokenizer one sentence or a list of sentences, so we can directly tokenize all the first sentences and all the second sentences of each pair like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rltjQ_MVQE4y"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "tokenized_sentence_1 = tokenizer(raw_datasets[\"train\"][\"sentence1\"])\n",
        "tokenized_sentence_2 = tokenizer(raw_datasets[\"train\"][\"sentence2\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjAo_IxqQlqE"
      },
      "source": [
        "However, we canâ€™t just pass two sequences to the model and get a prediction of whether the two sentences are paraphrases or not. We need to handle the two sequences as a pair, and apply the appropriate preprocessing. Fortunately, the tokenizer can also take a pair of sequences and prepare it the way our BERT model expects:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEQlewTyQnpH"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer(raw_train_dataset[86][\"sentence1\"], raw_train_dataset[86][\"sentence2\"])\n",
        "inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rudkp3Y8Q9Qa"
      },
      "source": [
        "In this example, the token_type_ids is what tells the model which part of the input is the first sentence and which is the second sentence.\n",
        "\n",
        "If we decode the IDs inside input_ids back to words:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9BfvJ_h_RnZT"
      },
      "outputs": [],
      "source": [
        "tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PROiqSS8SNH-"
      },
      "source": [
        "So we see the model expects the inputs to be of the form [CLS] sentence1 [SEP] sentence2 [SEP] when there are two sentences. Aligning this with the token_type_ids you can see, the parts of the input corresponding to [CLS] sentence1 [SEP] all have a token type ID of 0, while the other parts, corresponding to sentence2 [SEP], all have a token type ID of 1.\n",
        "\n",
        "Note that if you select a different checkpoint, you wonâ€™t necessarily have the token_type_ids in your tokenized inputs (for instance, theyâ€™re not returned if you use a DistilBERT model). They are only returned when the model will know what to do with them, because it has seen them during its pretraining.\n",
        "\n",
        "In general, you donâ€™t need to worry about whether or not there are token_type_ids in your tokenized inputs: as long as you use the same checkpoint for the tokenizer and the model, everything will be fine as the tokenizer knows what to provide to its model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Q90kqASUdcP"
      },
      "source": [
        "Now that we have seen how our tokenizer can deal with one pair of sentences, we can use it to tokenize our whole dataset: we can feed the tokenizer a list of pairs of sentences by giving it the list of first sentences, then the list of second sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCbHP6nxUirC"
      },
      "outputs": [],
      "source": [
        "tokenized_dataset = tokenizer(\n",
        "    raw_datasets[\"train\"][\"sentence1\"],\n",
        "    raw_datasets[\"train\"][\"sentence2\"],\n",
        "    padding=True,\n",
        "    truncation=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVzZ3JpOVQmN"
      },
      "source": [
        "This works well, but it has the disadvantage of returning a dictionary (with our keys, input_ids, attention_mask, and token_type_ids, and values that are lists of lists).\n",
        "\n",
        "To keep the data as a dataset, we will use the Dataset.map() method. This also allows us some extra flexibility, if we need more preprocessing done than just tokenization. The map() method works by applying a function on each element of the dataset, so letâ€™s define a function that tokenizes our inputs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rn79PLvrVVSq"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(example):\n",
        "  if 'sentence1' in example:\n",
        "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"],truncation=True)\n",
        "  else:\n",
        "    return tokenizer(example[\"sentence\"], truncation=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kCRO00kV1uy"
      },
      "source": [
        "Note that weâ€™ve left the padding argument out in our tokenization function for now. This is because padding all the samples to the maximum length is not efficient: itâ€™s better to pad the samples when weâ€™re building a batch, as then we only need to pad to the maximum length in that batch, and not the maximum length in the entire dataset. This can save a lot of time and processing power when the inputs have very variable lengths!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBnqs37lV7Rg"
      },
      "outputs": [],
      "source": [
        "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
        "tokenized_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63vNSEfIkg7i"
      },
      "outputs": [],
      "source": [
        "tokenized_datasets_sst = raw_datasets_sst.map(tokenize_function, batched=True)\n",
        "tokenized_datasets_sst"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7PnYreKYg6R"
      },
      "source": [
        "# Dynamic Padding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rYFNesPYi6d"
      },
      "source": [
        "The last thing we will need to do is pad all the examples to the length of the longest element when we batch elements together â€” a technique we refer to as dynamic padding.\n",
        "\n",
        "The function that is responsible for putting together samples inside a batch is called a collate function. Itâ€™s an argument you can pass when you build a DataLoader, the default being a function that will just convert your samples to PyTorch tensors and concatenate them (recursively if your elements are lists, tuples, or dictionaries). This wonâ€™t be possible in our case since the inputs we have wonâ€™t all be of the same size. We have deliberately postponed the padding, to only apply it as necessary on each batch and avoid having over-long inputs with a lot of padding. This will speed up training by quite a bit, but note that if youâ€™re training on a TPU it can cause problems â€” TPUs prefer fixed shapes, even when that requires extra padding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwOyTnuBY5BH"
      },
      "source": [
        "To do this in practice, we have to define a collate function that will apply the correct amount of padding to the items of the dataset we want to batch together. Fortunately, the ðŸ¤— Transformers library provides us with such a function via DataCollatorWithPadding. It takes a tokenizer when you instantiate it (to know which padding token to use, and whether the model expects padding to be on the left or on the right of the inputs) and will do everything you need:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvWPn5YyY59t"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHntMk5BhXDX"
      },
      "source": [
        "letâ€™s grab a few samples from our training set that we would like to batch together. Here, we remove the columns idx, sentence1, and sentence2 as they wonâ€™t be needed and contain strings (and we canâ€™t create tensors with strings) and have a look at the lengths of each entry in the batch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIeoBWoPgy_R"
      },
      "outputs": [],
      "source": [
        "samples = tokenized_datasets[\"train\"][:8]\n",
        "samples = {k: v for k, v in samples.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]}\n",
        "[len(x) for x in samples[\"input_ids\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNZ7P3ejhZ2Y"
      },
      "source": [
        "we get samples of varying length, from 32 to 67. Dynamic padding means the samples in this batch should all be padded to a length of 67, the maximum length inside the batch. Without dynamic padding, all of the samples would have to be padded to the maximum length in the whole dataset, or the maximum length the model can accept."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDeuenBEht2O"
      },
      "outputs": [],
      "source": [
        "batch = data_collator(samples)\n",
        "{k: v.shape for k,v in batch.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bnAp7SGnbhg"
      },
      "source": [
        "# Wrap preprocessing up\n",
        "\n",
        "Putting all together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ByAh3Qyvng66"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
        "\n",
        "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
        "raw_datasets_sst = load_dataset(\"glue\", \"sst2\")\n",
        "\n",
        "checkpoint = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "def tokenize_function(example):\n",
        "  if \"sentence\" in example:\n",
        "    return tokenizer(example[\"sentence\"], truncation=True)\n",
        "  else:\n",
        "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oz03TFQHoy1p"
      },
      "source": [
        "# Training\n",
        "\n",
        "ðŸ¤— Transformers provides a Trainer class to help you fine-tune any of the pretrained models it provides on your dataset. Once youâ€™ve done all the data preprocessing work in the last section, you have just a few steps left to define the Trainer. The hardest part is likely to be preparing the environment to run Trainer.train(), as it will run very slowly on a CPU. If you donâ€™t have a GPU set up, you can get access to free GPUs or TPUs on Google Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmzBabNJprnH"
      },
      "source": [
        "The first step before we can define our Trainer is to define a TrainingArguments class that will contain all the hyperparameters the Trainer will use for training and evaluation. The only argument you have to provide is a directory where the trained model will be saved, as well as the checkpoints along the way. For all the rest, you can leave the defaults, which should work pretty well for a basic fine-tuning.\n",
        "\n",
        "The second step is to define our model. As in the previous chapter, we will use the AutoModelForSequenceClassification class, with two labels:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pp3WDk92qir8"
      },
      "outputs": [],
      "source": [
        "! pip install -U accelerate\n",
        "! pip install -U transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3unfX9frpw-R"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\"test-trainer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgkRb9gmrOMY"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJ1cOH9Mua-c"
      },
      "source": [
        "Once we have our model, we can define a Trainer by passing it all the objects constructed up to now â€” the model, the training_args, the training and validation datasets, our data_collator, and our tokenizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjN6L85huclm"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sd-8NGyiu6qF"
      },
      "source": [
        "To fine-tune the model on our dataset, we just have to call the train() method of our Trainer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1MMIHCkmu7hb"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heuRWpgiu6j-"
      },
      "source": [
        "This will start the fine-tuning (which should take a couple of minutes on a GPU) and report the training loss every 500 steps. It wonâ€™t, however, tell you how well (or badly) your model is performing. This is because:\n",
        "\n",
        "   1. We didnâ€™t tell the Trainer to evaluate during training by setting evaluation_strategy to either \"steps\" (evaluate every eval_steps) or \"epoch\" (evaluate at the end of each epoch).\n",
        "   2. We didnâ€™t provide the Trainer with a compute_metrics() function to calculate a metric during said evaluation (otherwise the evaluation would just have printed the loss, which is not a very intuitive number)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBfHL3O1vQAj"
      },
      "source": [
        "# Evaluation\n",
        "\n",
        "Letâ€™s see how we can build a useful compute_metrics() function and use it the next time we train. The function must take an EvalPrediction object (which is a named tuple with a predictions field and a label_ids field) and will return a dictionary mapping strings to floats (the strings being the names of the metrics returned, and the floats their values). To get some predictions from our model, we can use the Trainer.predict() command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rD2FCaCivYqS"
      },
      "outputs": [],
      "source": [
        "predictions = trainer.predict(tokenized_datasets[\"validation\"])\n",
        "print(predictions.predictions.shape, predictions.label_ids.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output of the predict() method is another named tuple with three fields: predictions, label_ids, and metrics. The metrics field will just contain the loss on the dataset passed, as well as some time metrics.\n",
        "\n",
        "predictions is a two-dimensional array with shape 408 x 2 (408 being the number of elements in the dataset we used). Those are the logits for each element of the dataset we passed to predict(). To transform them into predictions that we can compare to our labels, we need to take the index with the maximum value on the second axis:"
      ],
      "metadata": {
        "id": "cAjcCSKWrDRt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "preds = np.argmax(predictions.predictions, axis=-1)"
      ],
      "metadata": {
        "id": "kkwi2tSCQxp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now compare those preds to the labels. To build our compute_metric() function, we will rely on the metrics from the ðŸ¤— Evaluate library. We can load the metrics associated with the MRPC dataset as easily as we loaded the dataset, this time with the evaluate.load() function. The object returned has a compute() method we can use to do the metric calculation:"
      ],
      "metadata": {
        "id": "Qm3FQ_WyrStL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"glue\", \"mrpc\")\n",
        "metric.compute(predictions=preds, references=predictions.label_ids)"
      ],
      "metadata": {
        "id": "DYQkyUfhq34L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wrapping everything together, we get our compute_metrics() function:"
      ],
      "metadata": {
        "id": "2m8KdeHErsD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_preds):\n",
        "  metric = evaluate.load(\"glue\", \"mrpc\")\n",
        "  logits, labels = eval_preds\n",
        "  predictions = np.argmax(logits, axis=-1)\n",
        "  return metric.compute(predictions=predictions, references=labels)"
      ],
      "metadata": {
        "id": "2v0-h-UFrt2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And to see it used in action to report metrics at the end of each epoch, here is how we define a new Trainer with this compute_metrics() function:"
      ],
      "metadata": {
        "id": "G1eHELKBsALk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model,\n",
        "    training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "tAetqOuEsDfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that we create a new TrainingArguments with its evaluation_strategy set to \"epoch\" and a new model â€” otherwise, we would just be continuing the training of the model we have already trained. To launch a new training run, we execute:"
      ],
      "metadata": {
        "id": "yjVzHkwXsfVR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "collapsed": true,
        "id": "3k0pNIyrsi6h",
        "outputId": "c283bf7d-02f8-4508-ea79-5467eeab41d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='23' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  23/1377 03:55 < 4:13:21, 0.09 it/s, Epoch 0.05/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This time, it will report the validation loss and metrics at the end of each epoch on top of the training loss.\n",
        "The Trainer will work out of the box on multiple GPUs or TPUs and provides lots of options, like mixed-precision training (use fp16 = True in your training arguments)."
      ],
      "metadata": {
        "id": "lhcIZ2UVsrEk"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMpgpzx9t+Ed3Czb/iG/sZh",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}